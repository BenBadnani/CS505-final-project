{"cells":[{"cell_type":"code","execution_count":1,"id":"ghIfS47AVBd1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghIfS47AVBd1","outputId":"8f536ea4-25b2-427f-8242-c1b62c3fe8d1","executionInfo":{"status":"ok","timestamp":1651429157897,"user_tz":240,"elapsed":5453,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bcolz in /usr/local/lib/python3.7/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.21.6)\n"]}],"source":["!pip install bcolz"]},{"cell_type":"code","execution_count":2,"id":"96da3e8a","metadata":{"id":"96da3e8a","executionInfo":{"status":"ok","timestamp":1651429162349,"user_tz":240,"elapsed":4456,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"outputs":[],"source":["import numpy as np \n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from google.colab import files\n","import bcolz\n","import pickle\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import csv\n","import json\n","import pandas as pd\n","from collections import Counter\n","import spacy\n","import re\n","nlp = spacy.load('en_core_web_sm')\n","#stopwords = nlp.Defaults.stop_words"]},{"cell_type":"code","execution_count":3,"id":"adKAoZQWWzm7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adKAoZQWWzm7","outputId":"9731e0a1-5af4-4e51-a0ab-5ef1287a16d2","executionInfo":{"status":"ok","timestamp":1651429163076,"user_tz":240,"elapsed":739,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"id":"YRMqlKGAny0z","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRMqlKGAny0z","outputId":"98fda675-42a9-42c7-89ce-cdf858b86c00","executionInfo":{"status":"ok","timestamp":1651429163215,"user_tz":240,"elapsed":143,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(96562, 9)\n"]}],"source":["path = '/content/drive/MyDrive/505/project/'\n","\n","data = pd.read_csv(f'{path}/data.csv')\n","print(data.shape)"]},{"cell_type":"code","source":["df = data.iloc[:96000]\n","print(df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVOX0u-4vq2j","executionInfo":{"status":"ok","timestamp":1651429163315,"user_tz":240,"elapsed":4,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"d166b9e2-463c-4127-910a-61c20bbc43a8"},"id":"OVOX0u-4vq2j","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(96000, 9)\n"]}]},{"cell_type":"code","source":["# Train test split\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df['retweets'], train_size = 71680, random_state=42)\n","print(len(X_train), len(X_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wm8Yp2ZqNJyI","executionInfo":{"status":"ok","timestamp":1651429165849,"user_tz":240,"elapsed":805,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"c8872eeb-2337-42b8-ee2b-f4332c9afc22"},"id":"Wm8Yp2ZqNJyI","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["71680 24320\n"]}]},{"cell_type":"code","source":["X_train_tweet = X_train.tweet\n","X_test_tweet = X_test.tweet\n","\n","X_train_meta = X_train.iloc[:,:-1]\n","X_test_meta = X_test.iloc[:,:-1]"],"metadata":{"id":"ZzutBlDAmyXP","executionInfo":{"status":"ok","timestamp":1651429169607,"user_tz":240,"elapsed":93,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"ZzutBlDAmyXP","execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Building vocabulary for training data\n","def word_count(data):\n","  words_counter = Counter()\n","  for line in data:\n","    words =  str(line).split()\n","    for w in words:\n","      words_counter.update([w])\n","  \n","  words_counter_clean = {k:v for k,v in words_counter.items() if v > 1} # Removing the words that only appear once\n","  sorted_words = sorted(words_counter_clean, key = words_counter_clean.get, reverse = True) # Sorting the words frequency in desc order\n","  sorted_words = ['UNK','PAD', '<s>', '</s>' ] + sorted_words \n","\n","  return words_counter, words_counter_clean, sorted_words\n","  \n","words_counter, words_counter_clean, sorted_words = word_count(X_train_tweet)"],"metadata":{"id":"uH6X4dcoOPLI","executionInfo":{"status":"ok","timestamp":1651429176434,"user_tz":240,"elapsed":5818,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"uH6X4dcoOPLI","execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Not using slicing window padding\n","def padding(data, seq_len):\n","  sequences = []\n","  for line in data:\n","    line = f\"{'<s>'} {line} {'</s>'}\"\n","    n_token = len(line.split())\n","    \n","    if n_token >= seq_len:\n","      seq = line.split()[:seq_len] \n","      sequences.append(\" \".join(seq))\n","\n","    else:\n","      seq = line.split()\n","      for i in range(seq_len - n_token):\n","          seq.append('PAD')\n","      sequences.append(\" \".join(seq))\n","  return sequences\n","\n","X_train_pad = padding(X_train_tweet, 20)\n","X_test_pad = padding(X_test_tweet, 20)"],"metadata":{"id":"QbqatJDhN9hj","executionInfo":{"status":"ok","timestamp":1651429178295,"user_tz":240,"elapsed":911,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"QbqatJDhN9hj","execution_count":9,"outputs":[]},{"cell_type":"code","source":["# replace the words that only appear once with UNKNOWN\n","def generate_sentence(data):\n","  sequences = []\n","  for line in data:\n","    temp = []\n","    words = line.split()\n","    for word in words:\n","      if word in sorted_words:\n","        temp.append(word)\n","      else:\n","        temp.append('UNK')\n","    sequences.append(\" \".join(temp))\n","  return sequences\n","\n","X_train_final = generate_sentence(X_train_pad)\n","X_test_final = generate_sentence(X_test_pad)"],"metadata":{"id":"8P30CPRaOfyW","executionInfo":{"status":"ok","timestamp":1651429208946,"user_tz":240,"elapsed":28055,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"8P30CPRaOfyW","execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"id":"o54nmNt2w4sd","metadata":{"id":"o54nmNt2w4sd","executionInfo":{"status":"ok","timestamp":1651429210532,"user_tz":240,"elapsed":695,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"outputs":[],"source":["# Using tweets training data vocabulary\n","\n","# Dictionaries to store the word to index mappings and vice versa\n","word2idx = {o:i for i,o in enumerate(sorted_words)}\n","idx2word = {i:o for i,o in enumerate(sorted_words)}\n","\n","\n","# convert text sequences to integer sequences\n","X_train_int = np.zeros((len(X_train_final), 20), dtype = int)\n","for i, data in enumerate(X_train_final):\n","  X_train_int[i] = [word2idx[w] for w in data.split()]\n","\n","X_test_int = np.zeros((len(X_test_final), 20), dtype = int)\n","for i, data in enumerate(X_test_final):\n","  X_test_int[i] = [word2idx[w] for w in data.split()]\n","\n"]},{"cell_type":"code","source":["# convert lists to numpy arrays\n","X_train_int = np.array(X_train_int)\n","y_train_int = np.array(y_train)\n","\n","X_test_int = np.array(X_test_int)\n","y_test_int = np.array(y_test)"],"metadata":{"id":"naTOqvd7Vg6m","executionInfo":{"status":"ok","timestamp":1651429213037,"user_tz":240,"elapsed":121,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"naTOqvd7Vg6m","execution_count":12,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(X_train_int), torch.from_numpy(X_train_meta.to_numpy()), torch.from_numpy(y_train_int))\n","test_data = TensorDataset(torch.from_numpy(X_test_int),torch.from_numpy(X_test_meta.to_numpy()), torch.from_numpy(y_test_int))\n","\n","# dataloaders\n","batch_size = 256\n","\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"],"metadata":{"id":"V526jI1Glo7o","executionInfo":{"status":"ok","timestamp":1651429215797,"user_tz":240,"elapsed":100,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"V526jI1Glo7o","execution_count":13,"outputs":[]},{"cell_type":"markdown","id":"7a3ac567","metadata":{"id":"7a3ac567"},"source":["# Glove Embeddings"]},{"cell_type":"code","source":["vectors = bcolz.open(f'{glove_path}/6B.100.dat')[:]\n","words = pickle.load(open(f'{glove_path}/6B.100_words.pkl', 'rb'))\n","words += ['<UNK>', '<s>', '</s>', 'PAD']\n","vocab_list_glove = set(words)\n","new_vecs = np.random.normal(loc=0.0, scale=.6, size=(4,100) )\n","vectors = np.vstack((vectors, new_vecs))\n","word2idx = pickle.load(open(f'{glove_path}/6B.100_idx.pkl', 'rb'))\n","word2idx['<UNK>'] = 400000\n","word2idx['<s>'] = 400001\n","word2idx['</s>'] = 400002\n","word2idx['PAD'] = 400003"],"metadata":{"id":"ZH76mVJhmzE5"},"id":"ZH76mVJhmzE5","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"6ii7TYo3rL1-","metadata":{"id":"6ii7TYo3rL1-"},"outputs":[],"source":["# Using glove weights\n","glove = {w: vectors[word2idx[w]] for w in words}\n","matrix_len = len(sorted_words)\n","weights_matrix = np.zeros((matrix_len, 100))\n","words_found = 0\n","\n","for i, word in enumerate(sorted_words):\n","  try: \n","    weights_matrix[i] = glove[word] # if alr in the vocab, load its pre-trained word vector.\n","    words_found += 1\n","  except KeyError:\n","    weights_matrix[i] = np.random.normal(scale=0.6, size=(100, ))"]},{"cell_type":"markdown","id":"5e5b8d40","metadata":{"id":"5e5b8d40"},"source":["# Neural Net\n","\n","### Retweet Network: Takes in a tweet as input, can use embedded version, and can any combination of bidirectional, LSTM, GRU, concatenates it with metadata vector, and uses a feedforward neural net with 1 hidden layer to perform a regression prediction on the retweet count. "]},{"cell_type":"markdown","id":"96c224d1","metadata":{"id":"96c224d1"},"source":["#### Parameter custom_embeddings is either a tuple: (weight_matrix , none_trainable), or None.\n","#### none_trainable is either True or False or Nothing"]},{"cell_type":"code","source":["def create_emb_layer(weights_matrix, non_trainable=False):\n","  num_embeddings, embedding_dim = weights_matrix.shape\n","  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n","  emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n","  if non_trainable:\n","      emb_layer.weight.requires_grad = False\n","  return emb_layer, num_embeddings, embedding_dim\n","\n","class RetweetNet(nn.Module):\n","  def __init__(self, vocab_size, hidden_state_sizes, meta_data_len, output_size, embedding_dim, hidden_dim, \n","                 n_layers, drop_prob=0.5, custom_embeddings = None, bidirectional = False, GRU = False):\n","    super().__init__()\n","    self.GRU_val = GRU\n","    self.bidirectional = bidirectional\n","    self.output_size = output_size\n","    self.n_layers = n_layers\n","    self.hidden_dim = hidden_dim\n","        \n","    if custom_embeddings is None: \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    else: \n","        assert len(custom_embeddings) == 2 and isinstance(custom_embeddings, tuple), \"custom embeddings must be of form: (weight_matrix, non_trainable)\"\n","        weights_matrix, non_trainable = custom_embeddings\n","        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, non_trainable)\n","        \n","    if GRU == False: \n","        self.Gate = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n","    else: \n","        self.Gate = nn.GRU(embedding_dim, hidden_dim, n_layers, \n","                              dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n","    self.dropout = nn.Dropout(0.2)\n","    self.fc1 = nn.Linear(hidden_dim, hidden_state_sizes[0])\n","    self.relu = nn.ReLU()\n","        \n","    #hidden_state_sizes[0] is the size of the output of lstm \n","    self.fc2 = nn.Linear(hidden_state_sizes[0] + meta_data_len, hidden_state_sizes[1])\n","        \n","    #hidden_state_sizes[1] is the size of the first and only hidden layer\n","    self.fc3 = nn.Linear(hidden_state_sizes[1], 1)\n","\n","        \n","  def forward(self, x, meta_data, hidden):\n","    batch_size = x.size(0)\n","    x = x.long()\n","    embeds = self.embedding(x)\n","    gru_out, hidden = self.Gate(embeds, hidden)\n","    gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n","    \n","    out = self.dropout(gru_out)\n","    out = self.fc1(out)\n","\n","    out = out.view(batch_size, -1, self.hidden_dim)\n","    out = out[:,-1, :] \n","\n","    # combine hidden state and meta_data\n","    ################# ################# #################\n","    #out = torch.cat((out, meta_data), dim = 1) #meta_data is of shape (batch_size, -1)\n","        \n","    out = self.fc2(out)\n","        \n","    # applying dropout before relu since relu already sets some neurons to 0\n","    out = self.dropout(out)\n","    out = self.relu(out)\n","    out = self.fc3(out)\n","        \n","    return out, hidden\n","    \n","  def init_hidden(self, batch_size):\n","    weight = next(self.parameters()).data\n","    n = 1\n","    if self.bidirectional == True: \n","      n = 2\n","    if self.GRU_val == False:\n","      return (weight.new(self.n_layers * n, batch_size, self.hidden_dim).zero_().to('cuda'),\n","              weight.new(self.n_layers * n, batch_size, self.hidden_dim).zero_().to('cuda'))\n","    else:\n","      return  weight.new(self.n_layers * n, batch_size, self.hidden_dim).zero_().to('cuda')\n","\n","\n","def train_retweet_predictor(model, epochs = 100, print_every = 1000, clip = 5, valid_loss_min = np.Inf, lr=0.005, batch_size = 400, device = 'cuda', GRU = False, weight_decay = 1e-5): \n","  counter = 0\n","  print_every = 200\n","  model.train()\n","    \n","  criterion = nn.MSELoss()\n","    \n","  # weight decay is the l2 regularization penalty \n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","\n","  training_stats = []\n","  for i in range(epochs):\n","    total_train_loss = 0\n","\n","    h = model.init_hidden(batch_size)\n","    for tweets, meta_data, labels in train_loader:\n","      counter += 1\n","      if GRU == False: \n","        h = tuple([each.data for each in h])\n","      else:\n","        h = h.data\n","      tweets, meta_data, labels = tweets.to(device), meta_data.to(device), labels.to(device)\n","      model.zero_grad()\n","      output, h = model(tweets, meta_data, h)\n","      loss = criterion(output.squeeze(), labels.float())\n","      loss.backward()\n","      total_train_loss += loss.item()\n","      nn.utils.clip_grad_norm_(model.parameters(), clip)\n","      optimizer.step()\n","\n","\n","      if counter % print_every == 0:\n","\n","        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n","            \"Step: {}...\".format(counter),\n","            \"Loss: {:.6f}...\".format(loss.item()))\n","      \n","\n","    #avg_train_loss = total_train_loss / len(train_loader)  \n","    #training_stats.append({'epoch': i + 1, 'Training Loss': avg_train_loss})\n","  \n","  #return training_stats\n","      \n","\n"],"metadata":{"id":"xGLQtrqBeoOr","executionInfo":{"status":"ok","timestamp":1651429242221,"user_tz":240,"elapsed":501,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"xGLQtrqBeoOr","execution_count":16,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(sorted_words)\n","output_size = len(X_train_int)\n","embedding_dim = 100\n","hidden_dim = 256\n","n_layers = 2\n","\n","net = RetweetNet(vocab_size = vocab_size, hidden_state_sizes = [256,128], meta_data_len = 0, output_size = output_size, embedding_dim = embedding_dim, hidden_dim  = hidden_dim, \n","                 n_layers =n_layers, GRU = True, bidirectional = True)\n","net.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqetYzck9STm","executionInfo":{"status":"ok","timestamp":1651429244263,"user_tz":240,"elapsed":237,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"670374f6-7823-4323-e829-cfa3f185a156"},"id":"DqetYzck9STm","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RetweetNet(\n","  (embedding): Embedding(15383, 100)\n","  (Gate): GRU(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc1): Linear(in_features=256, out_features=256, bias=True)\n","  (relu): ReLU()\n","  (fc2): Linear(in_features=256, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["train_retweet_predictor(net, epochs = 10, batch_size = 256, device = 'cuda', lr = 1e-06,GRU = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpeOpcKL-I9n","executionInfo":{"status":"ok","timestamp":1651429441219,"user_tz":240,"elapsed":189848,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"956877f0-25d7-4c04-df1c-f8953f6c4fae"},"id":"xpeOpcKL-I9n","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10... Step: 200... Loss: 141.335114...\n","Epoch: 2/10... Step: 400... Loss: 5251.428711...\n","Epoch: 3/10... Step: 600... Loss: 69.371346...\n","Epoch: 3/10... Step: 800... Loss: 83.891388...\n","Epoch: 4/10... Step: 1000... Loss: 288.821411...\n","Epoch: 5/10... Step: 1200... Loss: 180.430115...\n","Epoch: 5/10... Step: 1400... Loss: 117.800873...\n","Epoch: 6/10... Step: 1600... Loss: 139.857117...\n","Epoch: 7/10... Step: 1800... Loss: 266.878998...\n","Epoch: 8/10... Step: 2000... Loss: 243.920761...\n","Epoch: 8/10... Step: 2200... Loss: 5238.571777...\n","Epoch: 9/10... Step: 2400... Loss: 117.882454...\n","Epoch: 10/10... Step: 2600... Loss: 10010.580078...\n","Epoch: 10/10... Step: 2800... Loss: 43.863174...\n"]}]},{"cell_type":"code","source":["def error_retweet_predictor(model, batch_size = 359, device = 'cuda', GRU = False): \n","  test_losses = []\n","  num_correct = 0\n","  model.cuda()\n","\n","  h = model.init_hidden(batch_size)\n","  criterion = nn.MSELoss()\n","  \n","\n","  model.eval()\n","  for tweets, meta_data, labels in test_loader:\n","    if GRU == True: \n","      h = h.data\n","    else: \n","      h = tuple([each.data for each in h])\n","    tweets, meta_data, labels = tweets.to(device), meta_data.to(device), labels.to(device)\n","    output, h = model(tweets, meta_data, h)\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    pred = torch.round(output.squeeze())\n","    \n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","\n","    correct = np.squeeze(correct_tensor.cpu().numpy())\n","\n","    num_correct += np.sum(correct)   \n","   \n","  test_acc = num_correct/len(test_loader.dataset)\n","  print(num_correct, len(test_loader.dataset))\n","  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","  print(\"Test accuracy: {:.3f}\".format(test_acc))"],"metadata":{"id":"jy-20t06tV6m","executionInfo":{"status":"ok","timestamp":1651430526227,"user_tz":240,"elapsed":118,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}}},"id":"jy-20t06tV6m","execution_count":42,"outputs":[]},{"cell_type":"code","source":["error_retweet_predictor(net, batch_size = 256, device = 'cuda', GRU = True) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_QOdfST0z07","executionInfo":{"status":"ok","timestamp":1651430529539,"user_tz":240,"elapsed":2285,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"a2c02f20-b49b-492c-c1a7-9b62c59179d6"},"id":"r_QOdfST0z07","execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["2152 24320\n","Test loss: 58273.874\n","Test accuracy: 0.088\n"]}]},{"cell_type":"markdown","source":["### Visualize RetweetNet"],"metadata":{"id":"8Msdu-MQtJMM"},"id":"8Msdu-MQtJMM"},{"cell_type":"code","source":["!pip install torchviz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUzS4L7qOODm","executionInfo":{"status":"ok","timestamp":1651376775732,"user_tz":240,"elapsed":4599,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"dae79615-8ce4-4d9e-f1a1-633836bb6405"},"id":"WUzS4L7qOODm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.11.0+cu113)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.2.0)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=0108fda8d4a39f8e36420853eb1fcd356cf1ac971fcd7e5b8c065ef75d9066f5\n","  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]}]},{"cell_type":"code","source":["dummy_x = torch.from_numpy(np.zeros((256,20)).astype(np.int64)).to('cuda')\n","dummy_meta = torch.from_numpy(np.zeros((256,8)).astype(np.int64)).to('cuda')\n","dummy_hidden = torch.from_numpy(np.zeros((2, 256, 256)).astype(np.float32)).to('cuda')"],"metadata":{"id":"kfL246JWRrHC"},"id":"kfL246JWRrHC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchviz import make_dot\n","\n","yhat = net(dummy_x, dummy_meta, dummy_hidden)\n","make_dot(yhat, params = dict(list(net.named_parameters()) ), show_attrs=True,show_saved=True).render('something', format = 'png')"],"metadata":{"id":"NxSXNqWTu7X1"},"id":"NxSXNqWTu7X1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install hiddenlayer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qikHNSjqWemD","executionInfo":{"status":"ok","timestamp":1651376799370,"user_tz":240,"elapsed":4172,"user":{"displayName":"Nan Zhou","userId":"06252841090165992312"}},"outputId":"379baf40-3cfe-4b43-dd0d-e90ae7dfd63a"},"id":"qikHNSjqWemD","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hiddenlayer\n","  Downloading hiddenlayer-0.3-py3-none-any.whl (19 kB)\n","Installing collected packages: hiddenlayer\n","Successfully installed hiddenlayer-0.3\n"]}]},{"cell_type":"code","source":["import hiddenlayer as hl\n","\n","transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n","\n","graph = hl.build_graph(net, (dummy_x, dummy_meta, dummy_hidden), transforms=transforms)\n","graph.theme = hl.graph.THEMES['blue'].copy()\n","graph.save('rnn_hiddenlayer', format='png')"],"metadata":{"id":"7daHxCEPu-vS"},"id":"7daHxCEPu-vS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"buCLM2hT047v"},"id":"buCLM2hT047v","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Retweet Model clean.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}